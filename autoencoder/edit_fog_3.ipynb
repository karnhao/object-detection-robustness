{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab43c939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Resizing\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3c5511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_image(image, chunk_size):\n",
    "    h, w = image.shape[:2]\n",
    "    cw, ch = chunk_size\n",
    "    if h % ch != 0 or w % cw != 0:\n",
    "        raise ValueError(f\"Image size {w}x{h} not divisible by chunk {cw}x{ch}\")\n",
    "\n",
    "    chunks = []\n",
    "    for y in range(0, h, ch):\n",
    "        for x in range(0, w, cw):\n",
    "            chunk = image[y:y + ch, x:x + cw] / 255.0\n",
    "            chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def merge_chunks(chunks, image_size, chunk_size):\n",
    "    h, w = image_size\n",
    "    cw, ch = chunk_size\n",
    "    full_image = np.zeros((h, w, 3))\n",
    "    idx = 0\n",
    "\n",
    "    for y in range(0, h, ch):\n",
    "        for x in range(0, w, cw):\n",
    "            full_image[y:y + ch, x:x + cw] = chunks[idx]\n",
    "            idx += 1\n",
    "    return full_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7765483d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(input_shape):\n",
    "    input_img = Input(shape=input_shape)\n",
    "    h, w = input_shape[:2]\n",
    "\n",
    "    # Encoder\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(input_img)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(x)\n",
    "    encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = Conv2D(32, (3, 3), activation=None, padding='same')(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation=None, padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    x = Resizing(h, w)(x)\n",
    "\n",
    "    autoencoder = Model(input_img, x)\n",
    "    autoencoder.compile(optimizer=Adam(), loss=\"mse\")\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "919e904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(coco_data, clean_root_prefix, corruption_root,\n",
    "                   corruption_type, severity_level,\n",
    "                   chunk_size, full_size, batch_size=32):\n",
    "\n",
    "    corrupted_batch = []\n",
    "    clean_batch = []\n",
    "\n",
    "    for img_info in coco_data[\"images\"]:\n",
    "\n",
    "        relative_path = img_info[\"file_name\"]\n",
    "        clean_path = os.path.join(clean_root_prefix, relative_path)\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(relative_path))[0]\n",
    "        camera_dir = os.path.basename(os.path.dirname(relative_path))\n",
    "\n",
    "        corrupted_name = f\"{base_name}_{corruption_type}_{severity_level}.jpg\"\n",
    "        corrupted_path = os.path.join(corruption_root, camera_dir, corrupted_name)\n",
    "\n",
    "        clean = cv2.imread(clean_path)\n",
    "        corrupted = cv2.imread(corrupted_path)\n",
    "\n",
    "        if clean is None or corrupted is None:\n",
    "            continue\n",
    "\n",
    "        clean = cv2.resize(clean, full_size)\n",
    "        corrupted = cv2.resize(corrupted, full_size)\n",
    "\n",
    "        clean_chunks = split_image(clean, chunk_size)\n",
    "        corrupted_chunks = split_image(corrupted, chunk_size)\n",
    "\n",
    "        # Multi-chunk batching\n",
    "        for c, k in zip(corrupted_chunks, clean_chunks):\n",
    "            corrupted_batch.append(c)\n",
    "            clean_batch.append(k)\n",
    "\n",
    "            if len(corrupted_batch) == batch_size:\n",
    "                yield np.array(corrupted_batch), np.array(clean_batch)\n",
    "                corrupted_batch = []\n",
    "                clean_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aebc0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(\n",
    "        coco_json_path,\n",
    "        clean_root_prefix,\n",
    "        corruption_root,\n",
    "        corruption_type=\"fog\",\n",
    "        severity_level=2,\n",
    "        chunk_size=(200, 150),\n",
    "        full_size=(1600, 900),\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        weights_dir=\"weights\",\n",
    "):\n",
    "\n",
    "    os.makedirs(weights_dir, exist_ok=True)\n",
    "\n",
    "    # Load coco json metadata only (small)\n",
    "    with open(coco_json_path, \"r\") as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Compute input shape for model\n",
    "    sample_h = chunk_size[1]\n",
    "    sample_w = chunk_size[0]\n",
    "    input_shape = (sample_h, sample_w, 3)\n",
    "\n",
    "    # Build model\n",
    "    autoencoder = build_autoencoder(input_shape)\n",
    "\n",
    "    # Steps per epoch (48 chunks per full image)\n",
    "    chunks_per_image = (full_size[0] // chunk_size[0]) * (full_size[1] // chunk_size[1])\n",
    "    steps = (len(coco_data[\"images\"]) * chunks_per_image) // batch_size\n",
    "\n",
    "    print(f\"Total images: {len(coco_data['images'])}\")\n",
    "    print(f\"Chunks per image: {chunks_per_image}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Steps per epoch: {steps}\")\n",
    "\n",
    "    # Data generator (memory efficient)\n",
    "    train_gen = data_generator(\n",
    "        coco_data=coco_data,\n",
    "        clean_root_prefix=clean_root_prefix,\n",
    "        corruption_root=corruption_root,\n",
    "        corruption_type=corruption_type,\n",
    "        severity_level=severity_level,\n",
    "        chunk_size=chunk_size,\n",
    "        full_size=full_size,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    autoencoder.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=steps,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    weight_path = os.path.join(weights_dir, f\"autoencoder_{corruption_type}_{severity_level}.h5\")\n",
    "    autoencoder.save_weights(weight_path)\n",
    "    print(f\"Model weights saved to {weight_path}\")\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f258ff68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 1000\n",
      "Chunks per image: 48\n",
      "Batch size: 32\n",
      "Steps per epoch: 1500\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m corruption_root = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../data/sets/generated/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorruption_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m severity_level = \u001b[32m3\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model = \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoco_json_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoco_json_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclean_root_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclean_root_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorruption_root\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorruption_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcorruption_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcorruption_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseverity_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseverity_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1600\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m900\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_autoencoder\u001b[39m\u001b[34m(coco_json_path, clean_root_prefix, corruption_root, corruption_type, severity_level, chunk_size, full_size, epochs, batch_size, weights_dir)\u001b[39m\n\u001b[32m     38\u001b[39m train_gen = data_generator(\n\u001b[32m     39\u001b[39m     coco_data=coco_data,\n\u001b[32m     40\u001b[39m     clean_root_prefix=clean_root_prefix,\n\u001b[32m   (...)\u001b[39m\u001b[32m     46\u001b[39m     batch_size=batch_size\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mautoencoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m weight_path = os.path.join(weights_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mautoencoder_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorruption_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseverity_level\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     57\u001b[39m autoencoder.save_weights(weight_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\korea\\analyzing-corruption-perception\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\Desktop\\korea\\analyzing-corruption-perception\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\generator_data_adapter.py:16\u001b[39m, in \u001b[36mGeneratorDataAdapter.__init__\u001b[39m\u001b[34m(self, generator)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m._first_batches = first_batches\n\u001b[32m     15\u001b[39m \u001b[38;5;28mself\u001b[39m._output_signature = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mfirst_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     18\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWhen passing a Python generator to a Keras model, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe generator must return a tuple, either \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_batches[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    coco_json_path = \"../data/sets/nuimages/nuimages_1k.json\"\n",
    "    clean_root_prefix = \"../data/sets/nuimages\"\n",
    "    corruption_type = \"fog\"\n",
    "    corruption_root = f\"../data/sets/generated/{corruption_type}\"\n",
    "    severity_level = 3\n",
    "\n",
    "    model = train_autoencoder(\n",
    "        coco_json_path=coco_json_path,\n",
    "        clean_root_prefix=clean_root_prefix,\n",
    "        corruption_root=corruption_root,\n",
    "        corruption_type=corruption_type,\n",
    "        severity_level=severity_level,\n",
    "        full_size=(1600, 900),\n",
    "        chunk_size=(200, 150),\n",
    "        epochs=8,\n",
    "        batch_size=32\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92cc77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a654584c",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
